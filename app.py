import os
from dotenv import load_dotenv

# Carregar o ficheiro .env
load_dotenv()
print("‚úÖ Endpoint:", os.getenv("AZURE_OPENAI_ENDPOINT"))
print("‚úÖ Key:", os.getenv("AZURE_OPENAI_KEY")[:6] + "..." + os.getenv("AZURE_OPENAI_KEY")[-6:])
print("‚úÖ Version:", os.getenv("AZURE_OPENAI_API_VERSION"))
print("‚úÖ Deployment:", os.getenv("AZURE_OPENAI_DEPLOYMENT"))

import streamlit as st
# from langchain.vectorstores import Chroma
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain_openai import AzureChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from pathlib import Path
from PIL import Image

import unicodedata
import re

import base64
from io import BytesIO

from langchain.memory import ConversationBufferWindowMemory

# Inicializar mem√≥ria de janela com as √∫ltimas 4 mensagens
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferWindowMemory(
        k=4,  # n√∫mero de intera√ß√µes (user+AI) a manter
        return_messages=True
    )

memory = st.session_state.memory


def img_to_base64(img):
    buffered = BytesIO()
    img.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode()


# ======================
# 1Ô∏è‚É£ Azure Configura√ß√£o
# ======================
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY", "")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "ChatGPT")

llm = AzureChatOpenAI(
    temperature=0.7,
    model=AZURE_OPENAI_DEPLOYMENT,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_type="azure",
    api_key=AZURE_OPENAI_KEY,
    api_version=AZURE_OPENAI_API_VERSION
)

# ======================
# 2Ô∏è‚É£ Carregar documentos
# ======================
base_path = Path("rag_docs")
documentos = []

for folder in base_path.iterdir():
    if folder.is_dir():
        tipo = folder.name.lower()
        for file in folder.glob("*.txt"):
            with open(file, "r", encoding="utf-8") as f:
                conteudo = f.read()
            documentos.append({
                "nome": file.name,
                "conteudo": conteudo,
                "tipo": tipo,
                "id": file.stem.lower()
            })

# ======================
# 3Ô∏è‚É£ Criar chunks
# ======================
splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)

def dividir_em_chunks(documentos, splitter):
    chunks = []
    for doc in documentos:
        partes = splitter.split_text(doc["conteudo"])
        for i, parte in enumerate(partes):
            if len(parte.strip()) > 50:
                chunks.append({
                    "nome": doc["nome"],
                    "tipo": doc["tipo"],
                    "id": doc["id"],
                    "chunk_id": i,
                    "conteudo": parte
                })
    return chunks

document_chunks = dividir_em_chunks(documentos, splitter)

# ======================
# 4Ô∏è‚É£ Gerar embeddings e carregar base vetorial
# ======================
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

docs_formatados = [
    Document(page_content=chunk["conteudo"], metadata={
        "nome": chunk["nome"],
        "tipo": chunk["tipo"],
        "id": chunk["id"],
        "chunk_id": chunk["chunk_id"]
    }) for chunk in document_chunks
]

db_path = "chroma_db"
def criar_ou_carregar_faiss(docs, embedding):
    db = FAISS.from_documents(documents=docs, embedding=embedding)
    return db

chroma_db = criar_ou_carregar_faiss(docs_formatados, embedding_model)

# ======================
# 5Ô∏è‚É£ Fun√ß√µes RAG
# ======================

# ======================
# 
# ======================
def normalizar(texto):
    texto = texto.lower()
    texto = unicodedata.normalize('NFD', texto)
    texto = ''.join(c for c in texto if unicodedata.category(c) != 'Mn')
    return texto

def classificar_pergunta(pergunta):
    pergunta_norm = normalizar(pergunta)

    nomes_simples = {
        "vasco", "andre", "pato", "joao", "jota",
        "diogo", "rodrigo", "miranda", "ralph", "piloto", "paty"
    }

    nomes_compostos = {
        "monsieur ralph", "cara de ovo"
    }

    palavras_factuais = {
        "voo", "voos", "voar", "chegada", "chegar", "chegou",
        "partida", "partir", "data", "datas", "hora",
        "preco", "pre√ßo", "custos", "custo", "custaram",
        "valor", "pagamento", "quanto", "quantos",
        "aluguer", "seguro", "atividade", "atividades",
        "cruzeiro", "excursao", "passeio", "programa",
        "porto", "gruta", "bilhete", "confirmacao",
        "condutor", "condutores", "motorista", "carta"
    }

    palavras_pessoais = {
        "alcunha", "alcunhas", "apelido", "nome", "personagem", "quem √©"
    }

    # Detetar nomes simples com regex (palavras isoladas)
    if any(re.search(rf'\b{nome}\b', pergunta_norm) for nome in nomes_simples):
        return "pessoal"

    # Detetar nomes compostos ou termos pessoais com match direto
    if any(term in pergunta_norm for term in nomes_compostos.union(palavras_pessoais)):
        return "pessoal"

    if any(p in pergunta_norm for p in palavras_factuais):
        return "factual"

    return "generica"


# ======================
# 
# ======================
def rerank_heuristico(resultados_com_score, pergunta):
    pergunta_lower = pergunta.lower()

    # Termos t√≠picos de perguntas factuais
    termos_prioridade = [
        "voo", "voos", "ida", "regresso", "partida", "chegada",
        "escalas", "paris", "milao", "corfu", "aeroporto", "datas"
    ]

    # Nomes associados a participantes
    termos_pessoais = ["vasco", "andre", "pato", "joao", "jota", "diogo", "rodrigo", "miranda", "ralph"]

    # Inferir tipo de pergunta
    if any(t in pergunta_lower for t in termos_prioridade):
        tipo_esperado = "logistica"
    elif any(n in pergunta_lower for n in termos_pessoais):
        tipo_esperado = "participantes"
    else:
        tipo_esperado = None

    # Opcional: dete√ß√£o de nome pessoal mencionado (para refor√ßar chunks certos)
    nome_mencionado = next((n for n in termos_pessoais if n in pergunta_lower), None)

    def pontuar(r, score):
        bonus = 0

        # Pontos por termos factuais no conte√∫do
        bonus += 2 * sum(t in r.page_content.lower() for t in termos_prioridade)

        # Pontos por tipo correto (logistica ou participantes)
        if tipo_esperado and r.metadata.get("tipo") == tipo_esperado:
            bonus += 3

        # Pontos por id coincidir com nome mencionado
        if nome_mencionado and nome_mencionado in r.metadata.get("id", "").lower():
            bonus += 3

        return score - 0.1 * bonus

    return sorted(resultados_com_score, key=lambda r_score: pontuar(*r_score))
# ======================
# 
# ======================
def responder_factual(pergunta, resultados):
    memory = st.session_state.memory

    documentos = [r for r, _ in resultados]
    logistica_chunks = [r for r in documentos if r.metadata["tipo"] == "logistica"]
    termos_relevantes = ["voo", "seguro", "carro", "chegada", "partida", "corfu"]

    prioridade_chunks = [
        r for r in logistica_chunks
        if any(t in r.page_content.lower() for t in termos_relevantes)
    ]

    contexto = "\n\n".join(r.page_content for r in prioridade_chunks) if prioridade_chunks else "\n\n".join(r.page_content for r in logistica_chunks)

    # ‚úÖ Novo system_message com fluidez e precis√£o
    system_message = (
        "Est√°s a responder como um assistente de viagem do grupo que vai a Corfu em agosto de 2025. "
        "A conversa deve ser flu√≠da e natural ‚Äî usa o hist√≥rico anterior para perceber o contexto e dar continuidade. "
        "As tuas respostas devem ser diretas, claras e baseadas exclusivamente no contexto fornecido. "
        "N√£o inventes dados. Se n√£o houver informa√ß√£o suficiente, diz isso com simplicidade. "
        "Se a pergunta for vaga (ex: 'e o seguro?'), deves assumir que a conversa est√° em andamento e responder com base no que for mais relevante.\n\n"
        "Exemplos:\n"
        "- Pergunta: 'Qual √© o seguro do carro?' ‚Üí Resposta: 'O seguro √© da Collinson, custa 55,80‚Ç¨ e cobre danos e roubo de 8 a 14 de agosto em Corfu.'\n"
        "- Pergunta: 'Quanto custaram os carros?' ‚Üí Resposta: 'O aluguer dos carros foi de 117,40‚Ç¨ por pessoa.'\n"
        "- Pergunta: 'Quem s√£o os condutores do carro?' ‚Üí Resposta: 'Os condutores s√£o Rodrigo Miranda e Rodrigo Pato, conforme indicado na ap√≥lice do seguro.'"
    )

    # ‚úÖ Corrigir roles: transformar "human"/"ai" em "user"/"assistant"
    historico_formatado = []
    for m in memory.chat_memory.messages:
        if m.type == "human":
            historico_formatado.append({"role": "user", "content": m.content})
        elif m.type == "ai":
            historico_formatado.append({"role": "assistant", "content": m.content})

    # ‚úÖ Construir prompt completo com hist√≥rico
    mensagens = (
        [{"role": "system", "content": system_message}]
        + historico_formatado
        + [{"role": "user", "content": f"Pergunta: {pergunta}\n\nContexto:\n{contexto}"}]
    )

    resposta = llm.invoke(mensagens).content

    # ‚úÖ Atualizar mem√≥ria com nova intera√ß√£o
    memory.chat_memory.add_user_message(pergunta)
    memory.chat_memory.add_ai_message(resposta)
    st.session_state.mensagens = mensagens

    return resposta



def responder_pessoal(pergunta, resultados):
    memory = st.session_state.memory

    documentos = [r for r, _ in resultados]
    participantes_chunks = [r for r in documentos if r.metadata["tipo"] == "participantes"]

    mapa_nomes = {
        "vasco": "vasco_machado",
        "andre": "andre_marques",
        "andr√©": "andre_marques",
        "pato": "rodrigo_pato",
        "ralph": "rodrigo_pato",
        "joao": "joao_gomes",
        "jo√£o": "joao_gomes",
        "jota": "joao_gomes",
        "diogo": "diogo_miranda",
        "rodrigo": "rodrigo_miranda",
        "miranda": "rodrigo_miranda",
        "mosieur": "rodrigo_pato",
        "monsieur ralph": "rodrigo_pato",
        "cara de ovo": "diogo_miranda"
    }

    pergunta_lower = pergunta.lower()
    nomes_encontrados = [mapa_nomes[n] for n in mapa_nomes if n in pergunta_lower]
    if "paty" in pergunta_lower:
        nomes_encontrados = ["vasco_machado", "rodrigo_pato"]

    if nomes_encontrados:
        chunks_filtrados = [r for r in participantes_chunks if r.metadata["id"].lower() in nomes_encontrados]
    else:
        chunks_filtrados = participantes_chunks

    contexto = "\n\n".join(r.page_content for r in chunks_filtrados)
    ids_encontrados = set(r.metadata["id"].lower() for r in chunks_filtrados)

    personagens = []
    if "vasco_machado" in ids_encontrados:
        personagens.append("Vasco √© uma das Patys do grupo ‚Äî tem cabelo rapado mas decidiu lavar o cabelo antes da praia, e ainda pediu um batido tropical √† influencer. √â um brunch ambulante.")
    if "andre_marques" in ids_encontrados:
        personagens.append("Andr√© √© o piloto do grupo que s√≥ pensa em 'comer' gajas (ou seja, os avi√µes s√£o as mulheres que ele conhece na noite).")
    if "rodrigo_pato" in ids_encontrados:
        personagens.append("Rodrigo Pato √© a outra Paty ‚Äî sens√≠vel e dram√°tico, lavou o cabelo na piscina do hotel antes da praia e ficou traumatizado por isso.")
    if "joao_gomes" in ids_encontrados:
        personagens.append("Jota √© o bisonte de gin√°sio e o TVDE indiano.")
    if "diogo_miranda" in ids_encontrados:
        personagens.append("Diogo √© o influencer com passado alco√≥lico.")
    if "rodrigo_miranda" in ids_encontrados:
        personagens.append("Rodrigo Miranda √© o autista espiritual do grupo.")
    if not personagens:
        personagens.append("Est√°s a falar de um dos participantes da viagem, responde com humor e estilo pessoal.")

    personagem = "\n\n".join(personagens)

    if nomes_encontrados:
        if len(nomes_encontrados) == 1:
            descricao_nomes = f"o participante {nomes_encontrados[0].replace('_', ' ').title()}"
        else:
            lista_formatada = ', '.join(n.replace('_', ' ').title() for n in nomes_encontrados)
            descricao_nomes = f"os participantes {lista_formatada}"
    else:
        descricao_nomes = "um dos participantes"

    # ‚úÖ Novo system message mais natural e flu√≠do
    system_message = f"""
Est√°s a responder como um amigo muito pr√≥ximo deste grupo de viagem.

A conversa deve ser flu√≠da, divertida e com continuidade ‚Äî usa o hist√≥rico se fizer sentido. 
Se algu√©m disser "e o Vasco?", entende que a conversa j√° est√° em andamento e responde com base no que foi dito antes.

Usa o contexto e responde com piadas internas, exageros naturais e estilo real do grupo. 
Evita respostas rob√≥ticas ou polidas demais.

Se a pergunta for sobre {descricao_nomes}, deves:
- Refor√ßar os tra√ßos mais marcantes da personagem.
- Usar eventos reais das viagens (presentes no contexto).
- Inventar apenas se for plaus√≠vel e em linha com o estilo do grupo.
- Evitar repetir literalmente o conte√∫do ‚Äî interpreta-o de forma criativa.

Se n√£o souberes, responde com humor. Nunca digas que √©s uma IA.

{personagem}
"""

    # ‚úÖ Corrigir roles: transformar "human"/"ai" em "user"/"assistant"
    historico_formatado = []
    for m in memory.chat_memory.messages:
        if m.type == "human":
            historico_formatado.append({"role": "user", "content": m.content})
        elif m.type == "ai":
            historico_formatado.append({"role": "assistant", "content": m.content})

    mensagens = (
        [{"role": "system", "content": system_message}]
        + historico_formatado
        + [{"role": "user", "content": f"Pergunta: {pergunta}\n\nContexto:\n{contexto}"}]
    )

    resposta = llm.invoke(mensagens).content

    memory.chat_memory.add_user_message(pergunta)
    memory.chat_memory.add_ai_message(resposta)
    st.session_state.mensagens = mensagens

    return resposta



def responder_generico(pergunta, resultados):
    memory = st.session_state.memory  # Aceder √† mem√≥ria partilhada da sess√£o

    documentos = [r for r, _ in resultados]
    contexto = "\n\n".join(r.page_content for r in documentos[:5])

    # ‚úÖ Novo system message com √™nfase em fluidez
    system_message = (
        "Est√°s a responder como um assistente simp√°tico e √∫til do grupo de viagem a Corfu, em agosto de 2025. "
        "A conversa deve ser natural e flu√≠da ‚Äî usa o hist√≥rico sempre que for relevante. "
        "Se algu√©m te perguntar algo como 'e o seguro?', entende que a conversa j√° est√° em andamento. "
        "Responde com brevidade, clareza e simpatia. "
        "Se n√£o houver dados no contexto, diz isso de forma simples e direta. "
        "Evita criar personagens ou exagerar no humor, a n√£o ser que o tom da conversa j√° o esteja a pedir.\n\n"
        "Exemplo: Pergunta: 'O que posso perguntar aqui?' ‚Üí Resposta: 'Podes perguntar sobre voos, datas, seguro, carro alugado, atividades ou at√© sobre os participantes da viagem. Se quiseres, posso contar hist√≥rias deles com algum exagero √† mistura.'"
    )

    # ‚úÖ Corrigir roles: transformar "human"/"ai" em "user"/"assistant"
    historico_formatado = []
    for m in memory.chat_memory.messages:
        if m.type == "human":
            historico_formatado.append({"role": "user", "content": m.content})
        elif m.type == "ai":
            historico_formatado.append({"role": "assistant", "content": m.content})

    # ‚úÖ Construir prompt com hist√≥rico e contexto
    mensagens = (
        [{"role": "system", "content": system_message}]
        + historico_formatado
        + [{"role": "user", "content": f"Pergunta: {pergunta}\n\nContexto:\n{contexto}"}]
    )

    resposta = llm.invoke(mensagens).content

    # ‚úÖ Atualizar mem√≥ria com nova intera√ß√£o
    memory.chat_memory.add_user_message(pergunta)
    memory.chat_memory.add_ai_message(resposta)
    st.session_state.mensagens = mensagens

    return resposta




def responder_pergunta(pergunta, k=100):
    tipo_pergunta = classificar_pergunta(pergunta)
    resultados_com_score_raw = chroma_db.similarity_search_with_score(pergunta, k=k)
    resultados_com_score = rerank_heuristico(resultados_com_score_raw, pergunta)

    # log_diagnostico(pergunta, tipo_pergunta, resultados_com_score)


    if not resultados_com_score:
        return "Hmm, n√£o encontrei nada nos nossos ficheiros sobre isso. Podes tentar perguntar de outra forma ou sobre outra coisa?"

    if tipo_pergunta == "factual":
        return responder_factual(pergunta, resultados_com_score)
    elif tipo_pergunta == "pessoal":
        return responder_pessoal(pergunta, resultados_com_score)
    else:
        return responder_generico(pergunta, resultados_com_score)

# ======================
# 6Ô∏è‚É£ Interface Streamlit
# ======================

# üîÅ Inicializar hist√≥rico de chat e flag de primeira intera√ß√£o
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "primeira_interacao" not in st.session_state:
    st.session_state.primeira_interacao = True

tab1, tab2 = st.tabs(["üí¨ Chat", "üë• Membros"])

# ‚Äî‚Äî‚Äî TAB 1: Chat ‚Äî‚Äî‚Äî
with tab1:
    st.title("üå¥ Chatbot da Viagem a Corfu")

    # ‚úÖ Modo de depura√ß√£o para ver mem√≥ria da sess√£o
    debug_mode = st.sidebar.checkbox("üõ†Ô∏è Mostrar mem√≥ria da sess√£o", value=False)


    if not st.session_state.chat_history:
        st.info(
            "‚ö†Ô∏è *Este chatbot √© experimental e pode ter respostas erradas ou incompletas.*\n\n"
            "Se n√£o obtiveres a resposta que querias, tenta reformular a pergunta com outras palavras.\n\n"
            "Este chatbot n√£o tem mem√≥ria, cada mensagem deve ser feita de forma independente tendo em conta que o contexto n√£o ser√° considerado."
        )


    # Sugest√µes de pergunta (apenas na primeira intera√ß√£o)
    if st.session_state.primeira_interacao and not st.session_state.chat_history:
        st.markdown("üí° *Sugest√µes r√°pidas:*")
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("Quais s√£o os voos?"):
                st.session_state.primeira_interacao = False
                pergunta = "Quais s√£o os voos?"
                resposta = responder_pergunta(pergunta)
                st.session_state.chat_history.append({"role": "user", "content": pergunta})
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
                memory.chat_memory.add_user_message(pergunta)     # ‚úÖ Atualiza mem√≥ria
                memory.chat_memory.add_ai_message(resposta)       # ‚úÖ Atualiza mem√≥ria
                st.rerun()

        with col2:
            if st.button("Quem √© o cara de ovo?"):
                st.session_state.primeira_interacao = False
                pergunta = "Quem √© o cara de ovo?"
                resposta = responder_pergunta(pergunta)
                st.session_state.chat_history.append({"role": "user", "content": pergunta})
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
                memory.chat_memory.add_user_message(pergunta)     # ‚úÖ Atualiza mem√≥ria
                memory.chat_memory.add_ai_message(resposta)       # ‚úÖ Atualiza mem√≥ria
                st.rerun()


        with col3:
            if st.button("Quais s√£o as atividades?"):
                st.session_state.primeira_interacao = False
                pergunta = "Quais s√£o as atividades?"
                resposta = responder_pergunta(pergunta)
                st.session_state.chat_history.append({"role": "user", "content": pergunta})
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
                memory.chat_memory.add_user_message(pergunta)     # ‚úÖ Atualiza mem√≥ria
                memory.chat_memory.add_ai_message(resposta)       # ‚úÖ Atualiza mem√≥ria
                st.rerun()


    # Mostrar hist√≥rico 
    st.divider()
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ‚úÖ Mostrar conte√∫do da mem√≥ria, se debug_mode estiver ativo
    if debug_mode:
        memory = st.session_state.memory
        st.divider()

        st.sidebar.subheader("üìú Mem√≥ria formatada para o modelo (roles corrigidos)")
        for i, m in enumerate(memory.chat_memory.messages):
            role = "user" if m.type == "human" else "assistant"
            st.sidebar.markdown(f"**{i+1}. {role.upper()}**")
            st.sidebar.code(m.content, language="markdown")

        with st.expander("üß† Conte√∫do atual da mem√≥ria (LangChain)"):
            if not memory.chat_memory.messages:
                st.markdown("‚ùå *A mem√≥ria est√° vazia. Nada foi guardado.*")
            else:
                st.subheader("üì® LangChain Memory")
                for i, m in enumerate(memory.chat_memory.messages):
                    st.markdown(f"**{i+1}. {m.type.upper()}**")
                    st.code(m.content, language="markdown")

                st.subheader("üß± Representa√ß√£o bruta da mem√≥ria (debug avan√ßado)")
                for i, m in enumerate(memory.chat_memory.messages):
                    st.code(repr(m), language="python")

                if any(m.content.strip() == "" for m in memory.chat_memory.messages):
                    st.warning("‚ö†Ô∏è H√° mensagens com conte√∫do vazio na mem√≥ria.")

                if st.session_state.chat_history:
                    st.subheader("üßæ Hist√≥rico Visual (chat_history)")
                    for i, m in enumerate(st.session_state.chat_history):
                        st.markdown(f"**{i+1}. {m['role'].upper()}**")
                        st.code(m["content"], language="markdown")

                # üßπ Bot√£o para limpar tudo
                if st.button("üßπ Limpar mem√≥ria da sess√£o"):
                    memory.clear()
                    st.session_state.chat_history = []
                    st.session_state.mensagens = []
                    st.success("‚úÖ Mem√≥ria e hist√≥rico visual limpos com sucesso.")
                    st.rerun()

        # üì§ Mostrar prompt enviado ao modelo
        if "mensagens" in st.session_state and st.session_state.mensagens:
            st.sidebar.subheader("üì§ Prompt enviado ao modelo (√∫ltima intera√ß√£o)")
            for i, m in enumerate(st.session_state.mensagens):
                st.sidebar.markdown(f"**{i+1}. {m['role'].upper()}**")
                st.sidebar.code(m["content"], language="markdown")
        else:
            st.sidebar.markdown("‚ö†Ô∏è *O prompt enviado ao modelo ainda n√£o foi guardado.*")



    # Limpar input se sinalizador estiver ativo
    if "reset_input" in st.session_state and st.session_state.reset_input:
        st.session_state.pergunta_input = ""
        st.session_state.reset_input = False

    # Input com submit via Enter
    st.divider()
    with st.form(key="form_pergunta"):
        user_input = st.text_input(
            "‚úèÔ∏è Faz a tua pergunta:",
            placeholder="Escreve aqui a tua quest√£o...",
            key="pergunta_input"
        )
        submitted = st.form_submit_button("Enviar")

        # ‚úÖ NOVA VERS√ÉO (mant√©m o input limpo sem erro)
        if submitted and user_input.strip():
            pergunta = user_input  # guarda o valor antes de limpar
            st.session_state.primeira_interacao = False
            st.session_state.chat_history.append({"role": "user", "content": pergunta})
            resposta = responder_pergunta(pergunta)
            st.session_state.chat_history.append({"role": "assistant", "content": resposta})
            st.session_state.reset_input = True  # ativa sinalizador para limpar
            st.rerun()

        # ‚ùå VERS√ÉO ANTIGA (com erro, deixada aqui comentada como refer√™ncia)
        # if submitted and user_input.strip():
        #     st.session_state.primeira_interacao = False
        #     st.session_state.chat_history.append({"role": "user", "content": user_input})
        #     resposta = responder_pergunta(user_input)
        #     st.session_state.chat_history.append({"role": "assistant", "content": resposta})
        #     st.session_state.pergunta_input = ""  # limpa a caixa (isto d√° erro)
        #     st.rerun()




# ‚Äî‚Äî‚Äî TAB 2: Membros ‚Äî‚Äî‚Äî
with tab2:
    st.title("üë• Membros da Viagem")

    membros = [
        {"nome": "Jo√£o Gomes", "alcunha": "Ex da Ana Malta/TVDE Indiano", "ficheiro": "jota.jpeg"},
        {"nome": "Andr√© Marques", "alcunha": "IsoleOne", "ficheiro": "andre.jpeg"},
        {"nome": "Diogo Miranda", "alcunha": "Cara de Ovo", "ficheiro": "diogo.jpeg"},
        {"nome": "Rodrigo Miranda", "alcunha": "Rei", "ficheiro": "rodrigo.jpeg"},
        {"nome": "Rodrigo Pato", "alcunha": "Monsieur Ralph / Paty", "ficheiro": "pato.jpeg"},
        {"nome": "Vasco Machado", "alcunha": "Dumbo/Paty/Um dos Nha Bodi", "ficheiro": "vasco.jpeg"},
    ]

    cols = st.columns(3)


    # Mostrar os membros em linhas de 3
    for i in range(0, len(membros), 3):
        cols = st.columns(3)
        for j, membro in enumerate(membros[i:i+3]):
            with cols[j]:
                with st.container():
                    st.markdown(
                        f"""
                        <div style='
                            background-color: #2b2b2b;
                            padding: 15px;
                            border-radius: 12px;
                            text-align: center;
                            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
                            margin-bottom: 20px;
                        '>
                        """,
                        unsafe_allow_html=True
                    )

                    try:
                        img = Image.open(f"imagens/{membro['ficheiro']}")
                        st.markdown(
                            f"""
                            <img src="data:image/jpeg;base64,{img_to_base64(img)}"
                                style="width: 150px; border-radius: 10px; transition: 0.3s ease;
                                        box-shadow: 0 0 5px rgba(255,255,255,0.1);"
                                onmouseover="this.style.transform='scale(1.05)'; 
                                            this.style.boxShadow='0 0 15px rgba(138,255,255,0.5)'"
                                onmouseout="this.style.transform='scale(1)'; 
                                            this.style.boxShadow='0 0 5px rgba(255,255,255,0.1)'"
                            />
                            """,
                            unsafe_allow_html=True
                        )
                    except:
                        st.error(f"‚ùå Imagem n√£o encontrada: {membro['ficheiro']}")

                    st.markdown(f"**{membro['nome']}**")
                    st.markdown(f"<span style='color: #8ef; font-style: italic;'>{membro['alcunha']}</span>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)


