import os
from dotenv import load_dotenv

# Carregar o ficheiro .env
load_dotenv()
print("‚úÖ Endpoint:", os.getenv("AZURE_OPENAI_ENDPOINT"))
print("‚úÖ Key:", os.getenv("AZURE_OPENAI_KEY")[:6] + "..." + os.getenv("AZURE_OPENAI_KEY")[-6:])
print("‚úÖ Version:", os.getenv("AZURE_OPENAI_API_VERSION"))
print("‚úÖ Deployment:", os.getenv("AZURE_OPENAI_DEPLOYMENT"))

import streamlit as st
# from langchain.vectorstores import Chroma
from langchain.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain_openai import AzureChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from pathlib import Path
from PIL import Image

import unicodedata
import re

import base64
from io import BytesIO

# from streamlit_extras.cookies import get_cookie, set_cookie  ‚ùå (antigo e obsoleto)
from streamlit_cookies_manager import EncryptedCookieManager 

import uuid

import streamlit as st
from memoria import listar_conversas, carregar_conversa, guardar_conversa, apagar_conversa, apagar_todas_conversas

def gerar_nome_conversa_baseado_na_pergunta(pergunta):
    nome = pergunta.lower()
    nome = re.sub(r"[^\w\s]", "", nome)  # remove pontua√ß√£o
    nome = "_".join(nome.split()[:5])    # usa at√© 5 palavras
    return nome


# Criar user_id √∫nico se ainda n√£o existir
# üç™ Ler ou criar user_id persistente
cookies = EncryptedCookieManager(password=os.getenv("COOKIE_SECRET", "default123"))

if not cookies.ready():
    st.stop()  # Aguarda cookies estarem prontos

if not cookies.get("user_id"):
    new_id = str(uuid.uuid4())
    cookies["user_id"] = new_id
    cookies.save()
    user_id = new_id
else:
    user_id = cookies.get("user_id")

st.session_state.user_id = user_id

if "confirmar_apagar_tudo" not in st.session_state:
    st.session_state.confirmar_apagar_tudo = False

# Inicializar ID da conversa
if "conversa_id" not in st.session_state:
    st.session_state.conversa_id = "conversa_1"

# Inicializar lista de conversas se necess√°rio
if "conversas" not in st.session_state:
    st.session_state.conversas = {}


# Tentar carregar hist√≥rico do disco se ainda n√£o houver
if "chat_history" not in st.session_state:
    historico_guardado = carregar_conversa(
        st.session_state.user_id,
        st.session_state.conversa_id
    )
    st.session_state.chat_history = historico_guardado or []

# Carregar lista de conversas
conversas_disponiveis = listar_conversas(st.session_state.user_id)
# Preencher dicion√°rio com os nomes das conversas e respetivo conte√∫do (se quiseres usar mais tarde)
for c in conversas_disponiveis:
    if c not in st.session_state.conversas:
        st.session_state.conversas[c] = carregar_conversa(st.session_state.user_id, c) 

# Garante que a conversa atual est√° na lista
if st.session_state.conversa_id not in conversas_disponiveis:
    conversas_disponiveis = [st.session_state.conversa_id] + conversas_disponiveis


# Sidebar interativa
with st.sidebar:
    st.markdown("## üß† Sess√£o")
    st.write(f"üë§ User ID: `{st.session_state.user_id}`")
    st.write(f"üí¨ Conversa ID: `{st.session_state.conversa_id}`")

    # Seletor de conversas existentes
    conversa_escolhida = st.radio(
        "üìÅ Conversas:",
        conversas_disponiveis,
        index=conversas_disponiveis.index(st.session_state.conversa_id)
        if st.session_state.conversa_id in conversas_disponiveis else 0
    )

    # Trocar conversa
    if conversa_escolhida != st.session_state.conversa_id:
        st.session_state.conversa_id = conversa_escolhida
        st.session_state.chat_history = carregar_conversa(
            st.session_state.user_id,
            st.session_state.conversa_id
        )

    # Bot√£o Nova conversa
    if st.button("‚ûï Nova conversa"):
        # Guardar conversa atual antes de trocar
        guardar_conversa(
            st.session_state.user_id,
            st.session_state.conversa_id,
            st.session_state.chat_history
        )

        # Gerar novo nome √∫nico (conversa_1, conversa_2, etc)
        idx = 1
        while f"conversa_{idx}" in conversas_disponiveis:
            idx += 1
        novo_nome = f"conversa_{idx}"

        # Atualizar sess√£o
        st.session_state.conversa_id = novo_nome
        st.session_state.chat_history = []

        # Guardar nova conversa
        guardar_conversa(st.session_state.user_id, novo_nome, [])

        # Reset da flag de sugest√µes
        st.session_state.primeira_interacao = True

        # For√ßar refresh da app para atualizar lista
        st.rerun()

    # Bot√£o Limpar conversa atual
    if st.button("üßº Limpar conversa atual"):
        st.session_state.chat_history = []
        guardar_conversa(
            st.session_state.user_id,
            st.session_state.conversa_id,
            []
        )

    # Bot√£o Apagar conversa atual
    if st.button("üóëÔ∏è Apagar conversa atual"):
        apagar_conversa(
            st.session_state.user_id,
            st.session_state.conversa_id
        )

        # Atualizar estado da aplica√ß√£o
        conversas_restantes = listar_conversas(st.session_state.user_id)
        if conversas_restantes:
            nova_conversa = conversas_restantes[0]
            st.session_state.conversa_id = nova_conversa
            st.session_state.chat_history = carregar_conversa(
                st.session_state.user_id, nova_conversa
            )
        else:
            st.session_state.conversa_id = "conversa_1"
            st.session_state.chat_history = []
            guardar_conversa(st.session_state.user_id, "conversa_1", [])

        st.success("‚úÖ Conversa apagada com sucesso.")
        st.rerun()

    # Bot√£o Apagar todas as conversas
    with st.expander("‚ö†Ô∏è Op√ß√µes avan√ßadas"):
        st.markdown("### üî• Apagar todas as conversas")

        if st.button("‚ùå Quero apagar tudo"):
            st.session_state.confirmar_apagar_tudo = True

        if st.session_state.get("confirmar_apagar_tudo", False):
            st.warning("‚ö†Ô∏è Tens a certeza que queres apagar todas as conversas? Esta a√ß√£o n√£o pode ser revertida.")
            col1, col2 = st.columns([1, 2])

            with col1:
                if st.button("‚úÖ Sim, apagar tudo agora"):
                    apagar_todas_conversas(st.session_state.user_id)
                    st.session_state.conversa_id = "conversa_1"
                    st.session_state.chat_history = []
                    guardar_conversa(st.session_state.user_id, "conversa_1", [])
                    st.success("üß® Todas as conversas foram apagadas com sucesso.")
                    st.session_state.confirmar_apagar_tudo = False
                    st.rerun()

            with col2:
                if st.button("‚ùå Cancelar"):
                    st.session_state.confirmar_apagar_tudo = False



# Guardar sempre que houver hist√≥rico (podes mover para o fim do teu `chat handler`)
guardar_conversa(
    st.session_state.user_id,
    st.session_state.conversa_id,
    st.session_state.chat_history
)

from langchain.memory import ConversationBufferWindowMemory

# Inicializar mem√≥ria de janela com as √∫ltimas 4 mensagens
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferWindowMemory(
        k=4,  # n√∫mero de intera√ß√µes (user+AI) a manter
        return_messages=True
    )

memory = st.session_state.memory


def img_to_base64(img):
    buffered = BytesIO()
    img.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode()


# ======================
# 1Ô∏è‚É£ Azure Configura√ß√£o
# ======================
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY", "")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
AZURE_OPENAI_DEPLOYMENT = os.getenv("AZURE_OPENAI_DEPLOYMENT", "ChatGPT")

llm = AzureChatOpenAI(
    temperature=0.7,
    model=AZURE_OPENAI_DEPLOYMENT,
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    openai_api_type="azure",
    api_key=AZURE_OPENAI_KEY,
    api_version=AZURE_OPENAI_API_VERSION
)

# ======================
# 2Ô∏è‚É£ Carregar documentos
# ======================
base_path = Path("rag_docs")
documentos = []

for folder in base_path.iterdir():
    if folder.is_dir():
        tipo = folder.name.lower()
        for file in folder.glob("*.txt"):
            with open(file, "r", encoding="utf-8") as f:
                conteudo = f.read()
            documentos.append({
                "nome": file.name,
                "conteudo": conteudo,
                "tipo": tipo,
                "id": file.stem.lower()
            })

# ======================
# 3Ô∏è‚É£ Criar chunks
# ======================
splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)

def dividir_em_chunks(documentos, splitter):
    chunks = []
    for doc in documentos:
        partes = splitter.split_text(doc["conteudo"])
        for i, parte in enumerate(partes):
            if len(parte.strip()) > 50:
                chunks.append({
                    "nome": doc["nome"],
                    "tipo": doc["tipo"],
                    "id": doc["id"],
                    "chunk_id": i,
                    "conteudo": parte
                })
    return chunks

document_chunks = dividir_em_chunks(documentos, splitter)

# ======================
# 4Ô∏è‚É£ Gerar embeddings e carregar base vetorial
# ======================
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}
)


docs_formatados = [
    Document(page_content=chunk["conteudo"], metadata={
        "nome": chunk["nome"],
        "tipo": chunk["tipo"],
        "id": chunk["id"],
        "chunk_id": chunk["chunk_id"]
    }) for chunk in document_chunks
]

db_path = "chroma_db"
def criar_ou_carregar_faiss(docs, embedding):
    db = FAISS.from_documents(documents=docs, embedding=embedding)
    return db

chroma_db = criar_ou_carregar_faiss(docs_formatados, embedding_model)

# ======================
# 5Ô∏è‚É£ Fun√ß√µes RAG
# ======================

# ======================
# 
# ======================
def normalizar(texto):
    texto = texto.lower()
    texto = unicodedata.normalize('NFD', texto)
    texto = ''.join(c for c in texto if unicodedata.category(c) != 'Mn')
    return texto

def classificar_pergunta(pergunta):
    pergunta_norm = normalizar(pergunta)

    nomes_simples = {
        "vasco", "andre", "pato", "joao", "jota",
        "diogo", "rodrigo", "miranda", "ralph", "piloto", "paty"
    }

    nomes_compostos = {
        "monsieur ralph", "cara de ovo"
    }

    palavras_factuais = {
        "voo", "voos", "voar", "chegada", "chegar", "chegou",
        "partida", "partir", "data", "datas", "hora",
        "preco", "pre√ßo", "custos", "custo", "custaram",
        "valor", "pagamento", "quanto", "quantos",
        "aluguer", "seguro", "atividade", "atividades",
        "cruzeiro", "excursao", "passeio", "programa",
        "porto", "gruta", "bilhete", "confirmacao",
        "condutor", "condutores", "motorista", "carta"
    }

    palavras_pessoais = {
        "alcunha", "alcunhas", "apelido", "nome", "personagem", "quem √©"
    }

    # Detetar nomes simples com regex (palavras isoladas)
    if any(re.search(rf'\b{nome}\b', pergunta_norm) for nome in nomes_simples):
        return "pessoal"

    # Detetar nomes compostos ou termos pessoais com match direto
    if any(term in pergunta_norm for term in nomes_compostos.union(palavras_pessoais)):
        return "pessoal"

    if any(p in pergunta_norm for p in palavras_factuais):
        return "factual"

    return "generica"


# ======================
# 
# ======================
def rerank_heuristico(resultados_com_score, pergunta):
    pergunta_lower = pergunta.lower()

    # Termos t√≠picos de perguntas factuais
    termos_prioridade = [
        "voo", "voos", "ida", "regresso", "partida", "chegada",
        "escalas", "paris", "milao", "corfu", "aeroporto", "datas"
    ]

    # Nomes associados a participantes
    termos_pessoais = ["vasco", "andre", "pato", "joao", "jota", "diogo", "rodrigo", "miranda", "ralph"]

    # Inferir tipo de pergunta
    if any(t in pergunta_lower for t in termos_prioridade):
        tipo_esperado = "logistica"
    elif any(n in pergunta_lower for n in termos_pessoais):
        tipo_esperado = "participantes"
    else:
        tipo_esperado = None

    # Opcional: dete√ß√£o de nome pessoal mencionado (para refor√ßar chunks certos)
    nome_mencionado = next((n for n in termos_pessoais if n in pergunta_lower), None)

    def pontuar(r, score):
        bonus = 0

        # Pontos por termos factuais no conte√∫do
        bonus += 2 * sum(t in r.page_content.lower() for t in termos_prioridade)

        # Pontos por tipo correto (logistica ou participantes)
        if tipo_esperado and r.metadata.get("tipo") == tipo_esperado:
            bonus += 3

        # Pontos por id coincidir com nome mencionado
        if nome_mencionado and nome_mencionado in r.metadata.get("id", "").lower():
            bonus += 3

        return score - 0.1 * bonus

    return sorted(resultados_com_score, key=lambda r_score: pontuar(*r_score))
# ======================
# 
# ======================
def responder_factual(pergunta, resultados):
    memory = st.session_state.memory

    documentos = [r for r, _ in resultados]
    logistica_chunks = [r for r in documentos if r.metadata["tipo"] == "logistica"]
    termos_relevantes = ["voo", "seguro", "carro", "chegada", "partida", "corfu"]

    prioridade_chunks = [
        r for r in logistica_chunks
        if any(t in r.page_content.lower() for t in termos_relevantes)
    ]

    contexto = "\n\n".join(r.page_content for r in prioridade_chunks) if prioridade_chunks else "\n\n".join(r.page_content for r in logistica_chunks)

    # ‚úÖ Novo system_message com fluidez e precis√£o
    system_message = (
        "Est√°s a responder como um assistente de viagem do grupo que vai a Corfu em agosto de 2025. "
        "A conversa deve ser flu√≠da e natural ‚Äî usa o hist√≥rico anterior para perceber o contexto e dar continuidade. "
        "As tuas respostas devem ser diretas, claras e baseadas exclusivamente no contexto fornecido. "
        "N√£o inventes dados. Se n√£o houver informa√ß√£o suficiente, diz isso com simplicidade. "
        "Se a pergunta for vaga (ex: 'e o seguro?'), deves assumir que a conversa est√° em andamento e responder com base no que for mais relevante.\n\n"
        "Exemplos:\n"
        "- Pergunta: 'Qual √© o seguro do carro?' ‚Üí Resposta: 'O seguro √© da Collinson, custa 55,80‚Ç¨ e cobre danos e roubo de 8 a 14 de agosto em Corfu.'\n"
        "- Pergunta: 'Quanto custaram os carros?' ‚Üí Resposta: 'O aluguer dos carros foi de 117,40‚Ç¨ por pessoa.'\n"
        "- Pergunta: 'Quem s√£o os condutores do carro?' ‚Üí Resposta: 'Os condutores s√£o Rodrigo Miranda e Rodrigo Pato, conforme indicado na ap√≥lice do seguro.'"
    )

    # ‚úÖ Corrigir roles: transformar "human"/"ai" em "user"/"assistant"
    historico_formatado = []
    for m in memory.chat_memory.messages:
        if m.type == "human":
            historico_formatado.append({"role": "user", "content": m.content})
        elif m.type == "ai":
            historico_formatado.append({"role": "assistant", "content": m.content})

    # ‚úÖ Construir prompt completo com hist√≥rico
    mensagens = (
        [{"role": "system", "content": system_message}]
        + historico_formatado
        + [{"role": "user", "content": f"Pergunta: {pergunta}\n\nContexto:\n{contexto}"}]
    )

    resposta = llm.invoke(mensagens).content

    # ‚úÖ Atualizar mem√≥ria com nova intera√ß√£o
    memory.chat_memory.add_user_message(pergunta)
    memory.chat_memory.add_ai_message(resposta)
    st.session_state.mensagens = mensagens

    return resposta



def responder_pessoal(pergunta, resultados):
    memory = st.session_state.memory

    documentos = [r for r, _ in resultados]
    participantes_chunks = [r for r in documentos if r.metadata["tipo"] == "participantes"]

    mapa_nomes = {
        "vasco": "vasco_machado",
        "andre": "andre_marques",
        "andr√©": "andre_marques",
        "pato": "rodrigo_pato",
        "ralph": "rodrigo_pato",
        "joao": "joao_gomes",
        "jo√£o": "joao_gomes",
        "jota": "joao_gomes",
        "diogo": "diogo_miranda",
        "rodrigo": "rodrigo_miranda",
        "miranda": "rodrigo_miranda",
        "mosieur": "rodrigo_pato",
        "monsieur ralph": "rodrigo_pato",
        "cara de ovo": "diogo_miranda"
    }

    pergunta_lower = pergunta.lower()
    nomes_encontrados = [mapa_nomes[n] for n in mapa_nomes if n in pergunta_lower]
    if "paty" in pergunta_lower:
        nomes_encontrados = ["vasco_machado", "rodrigo_pato"]

    if nomes_encontrados:
        chunks_filtrados = [r for r in participantes_chunks if r.metadata["id"].lower() in nomes_encontrados]
    else:
        chunks_filtrados = participantes_chunks

    contexto = "\n\n".join(r.page_content for r in chunks_filtrados)
    ids_encontrados = set(r.metadata["id"].lower() for r in chunks_filtrados)

    personagens = []
    if "vasco_machado" in ids_encontrados:
        personagens.append("Vasco √© uma das Patys do grupo ‚Äî tem cabelo rapado mas decidiu lavar o cabelo antes da praia, e ainda pediu um batido tropical √† influencer. √â um brunch ambulante.")
    if "andre_marques" in ids_encontrados:
        personagens.append("Andr√© √© o piloto do grupo que s√≥ pensa em 'comer' gajas (ou seja, os avi√µes s√£o as mulheres que ele conhece na noite).")
    if "rodrigo_pato" in ids_encontrados:
        personagens.append("Rodrigo Pato √© a outra Paty ‚Äî sens√≠vel e dram√°tico, lavou o cabelo na piscina do hotel antes da praia e ficou traumatizado por isso.")
    if "joao_gomes" in ids_encontrados:
        personagens.append("Jota √© o bisonte de gin√°sio e o TVDE indiano.")
    if "diogo_miranda" in ids_encontrados:
        personagens.append("Diogo √© o influencer com passado alco√≥lico.")
    if "rodrigo_miranda" in ids_encontrados:
        personagens.append("Rodrigo Miranda √© o autista espiritual do grupo.")
    if not personagens:
        personagens.append("Est√°s a falar de um dos participantes da viagem, responde com humor e estilo pessoal.")

    personagem = "\n\n".join(personagens)

    if nomes_encontrados:
        if len(nomes_encontrados) == 1:
            descricao_nomes = f"o participante {nomes_encontrados[0].replace('_', ' ').title()}"
        else:
            lista_formatada = ', '.join(n.replace('_', ' ').title() for n in nomes_encontrados)
            descricao_nomes = f"os participantes {lista_formatada}"
    else:
        descricao_nomes = "um dos participantes"

    # ‚úÖ Novo system message mais natural e flu√≠do
    system_message = f"""
Est√°s a responder como um amigo muito pr√≥ximo deste grupo de viagem.

A conversa deve ser flu√≠da, divertida e com continuidade ‚Äî usa o hist√≥rico se fizer sentido. 
Se algu√©m disser "e o Vasco?", entende que a conversa j√° est√° em andamento e responde com base no que foi dito antes.

Usa o contexto e responde com piadas internas, exageros naturais e estilo real do grupo. 
Evita respostas rob√≥ticas ou polidas demais.

Se a pergunta for sobre {descricao_nomes}, deves:
- Refor√ßar os tra√ßos mais marcantes da personagem.
- Usar eventos reais das viagens (presentes no contexto).
- Inventar apenas se for plaus√≠vel e em linha com o estilo do grupo.
- Evitar repetir literalmente o conte√∫do ‚Äî interpreta-o de forma criativa.

Se n√£o souberes, responde com humor. Nunca digas que √©s uma IA.

{personagem}
"""

    # ‚úÖ Corrigir roles: transformar "human"/"ai" em "user"/"assistant"
    historico_formatado = []
    for m in memory.chat_memory.messages:
        if m.type == "human":
            historico_formatado.append({"role": "user", "content": m.content})
        elif m.type == "ai":
            historico_formatado.append({"role": "assistant", "content": m.content})

    mensagens = (
        [{"role": "system", "content": system_message}]
        + historico_formatado
        + [{"role": "user", "content": f"Pergunta: {pergunta}\n\nContexto:\n{contexto}"}]
    )

    resposta = llm.invoke(mensagens).content

    memory.chat_memory.add_user_message(pergunta)
    memory.chat_memory.add_ai_message(resposta)
    st.session_state.mensagens = mensagens

    return resposta



def responder_generico(pergunta, resultados):
    memory = st.session_state.memory  # Aceder √† mem√≥ria partilhada da sess√£o

    documentos = [r for r, _ in resultados]
    contexto = "\n\n".join(r.page_content for r in documentos[:5])

    # ‚úÖ Novo system message com √™nfase em fluidez
    system_message = (
        "Est√°s a responder como um assistente simp√°tico e √∫til do grupo de viagem a Corfu, em agosto de 2025. "
        "A conversa deve ser natural e flu√≠da ‚Äî usa o hist√≥rico sempre que for relevante. "
        "Se algu√©m te perguntar algo como 'e o seguro?', entende que a conversa j√° est√° em andamento. "
        "Responde com brevidade, clareza e simpatia. "
        "Se n√£o houver dados no contexto, diz isso de forma simples e direta. "
        "Evita criar personagens ou exagerar no humor, a n√£o ser que o tom da conversa j√° o esteja a pedir.\n\n"
        "Exemplo: Pergunta: 'O que posso perguntar aqui?' ‚Üí Resposta: 'Podes perguntar sobre voos, datas, seguro, carro alugado, atividades ou at√© sobre os participantes da viagem. Se quiseres, posso contar hist√≥rias deles com algum exagero √† mistura.'"
    )

    # ‚úÖ Corrigir roles: transformar "human"/"ai" em "user"/"assistant"
    historico_formatado = []
    for m in memory.chat_memory.messages:
        if m.type == "human":
            historico_formatado.append({"role": "user", "content": m.content})
        elif m.type == "ai":
            historico_formatado.append({"role": "assistant", "content": m.content})

    # ‚úÖ Construir prompt com hist√≥rico e contexto
    mensagens = (
        [{"role": "system", "content": system_message}]
        + historico_formatado
        + [{"role": "user", "content": f"Pergunta: {pergunta}\n\nContexto:\n{contexto}"}]
    )

    resposta = llm.invoke(mensagens).content

    # ‚úÖ Atualizar mem√≥ria com nova intera√ß√£o
    memory.chat_memory.add_user_message(pergunta)
    memory.chat_memory.add_ai_message(resposta)
    st.session_state.mensagens = mensagens

    return resposta




def responder_pergunta(pergunta, k=100):
    tipo_pergunta = classificar_pergunta(pergunta)
    resultados_com_score_raw = chroma_db.similarity_search_with_score(pergunta, k=k)
    resultados_com_score = rerank_heuristico(resultados_com_score_raw, pergunta)

    # log_diagnostico(pergunta, tipo_pergunta, resultados_com_score)


    if not resultados_com_score:
        return "Hmm, n√£o encontrei nada nos nossos ficheiros sobre isso. Podes tentar perguntar de outra forma ou sobre outra coisa?"

    if tipo_pergunta == "factual":
        return responder_factual(pergunta, resultados_com_score)
    elif tipo_pergunta == "pessoal":
        return responder_pessoal(pergunta, resultados_com_score)
    else:
        return responder_generico(pergunta, resultados_com_score)

# ======================
# 6Ô∏è‚É£ Interface Streamlit
# ======================

# üîÅ Inicializar hist√≥rico de chat e flag de primeira intera√ß√£o
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "primeira_interacao" not in st.session_state:
    st.session_state.primeira_interacao = True

tab1, tab2 = st.tabs(["üí¨ Chat", "üë• Membros"])

# ‚Äî‚Äî‚Äî TAB 1: Chat ‚Äî‚Äî‚Äî
with tab1:
    st.title("üå¥ Chatbot da Viagem a Corfu")

    # ‚úÖ Modo de depura√ß√£o para ver mem√≥ria da sess√£o
    debug_mode = st.sidebar.checkbox("üõ†Ô∏è Mostrar mem√≥ria da sess√£o", value=False)


    if not st.session_state.chat_history:
        st.info(
            "‚ö†Ô∏è *Este chatbot √© experimental e pode ter respostas erradas ou incompletas.*\n\n"
            "Se n√£o obtiveres a resposta que querias, tenta reformular a pergunta com outras palavras."
        )


    # Sugest√µes de pergunta (apenas na primeira intera√ß√£o)
    if st.session_state.primeira_interacao and not st.session_state.chat_history:
        st.markdown("üí° *Sugest√µes r√°pidas:*")
        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("Quais s√£o os voos?"):
                st.session_state.primeira_interacao = False
                pergunta = "Quais s√£o os voos?"
                resposta = responder_pergunta(pergunta)
                st.session_state.chat_history.append({"role": "user", "content": pergunta})
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
                memory.chat_memory.add_user_message(pergunta)
                memory.chat_memory.add_ai_message(resposta)

                if st.session_state.conversa_id.startswith("conversa_"):
                    novo_nome = gerar_nome_conversa_baseado_na_pergunta(pergunta)
                    pasta_user = f"historico_data/user_{st.session_state.user_id}"
                    caminho_antigo = os.path.join(pasta_user, f"{st.session_state.conversa_id}.json")
                    caminho_novo = os.path.join(pasta_user, f"{novo_nome}.json")

                    if not os.path.exists(caminho_novo):
                        os.rename(caminho_antigo, caminho_novo)

                        if "conversas" in st.session_state:
                            st.session_state.conversas[novo_nome] = st.session_state.conversas.pop(
                                st.session_state.conversa_id, []
                            )

                        st.session_state.conversa_id = novo_nome

                st.rerun()


        with col2:
            if st.button("Quem √© o cara de ovo?"):
                st.session_state.primeira_interacao = False
                pergunta = "Quem √© o cara de ovo?"
                resposta = responder_pergunta(pergunta)
                st.session_state.chat_history.append({"role": "user", "content": pergunta})
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
                memory.chat_memory.add_user_message(pergunta)
                memory.chat_memory.add_ai_message(resposta)

                # ‚úÖ RENOMEAR a conversa se ainda tiver nome gen√©rico
                if st.session_state.conversa_id.startswith("conversa_"):
                    novo_nome = gerar_nome_conversa_baseado_na_pergunta(pergunta)
                    pasta_user = f"historico_data/user_{st.session_state.user_id}"
                    caminho_antigo = os.path.join(pasta_user, f"{st.session_state.conversa_id}.json")
                    caminho_novo = os.path.join(pasta_user, f"{novo_nome}.json")

                    if not os.path.exists(caminho_novo):
                        os.rename(caminho_antigo, caminho_novo)

                        if "conversas" in st.session_state:
                            st.session_state.conversas[novo_nome] = st.session_state.conversas.pop(
                                st.session_state.conversa_id, []
                            )

                        st.session_state.conversa_id = novo_nome

                st.rerun()

        with col3:
            if st.button("Quais s√£o as atividades?"):
                st.session_state.primeira_interacao = False
                pergunta = "Quais s√£o as atividades?"
                resposta = responder_pergunta(pergunta)
                st.session_state.chat_history.append({"role": "user", "content": pergunta})
                st.session_state.chat_history.append({"role": "assistant", "content": resposta})
                memory.chat_memory.add_user_message(pergunta)
                memory.chat_memory.add_ai_message(resposta)

                if st.session_state.conversa_id.startswith("conversa_"):
                    novo_nome = gerar_nome_conversa_baseado_na_pergunta(pergunta)
                    pasta_user = f"historico_data/user_{st.session_state.user_id}"
                    caminho_antigo = os.path.join(pasta_user, f"{st.session_state.conversa_id}.json")
                    caminho_novo = os.path.join(pasta_user, f"{novo_nome}.json")

                    if not os.path.exists(caminho_novo):
                        os.rename(caminho_antigo, caminho_novo)

                        if "conversas" in st.session_state:
                            st.session_state.conversas[novo_nome] = st.session_state.conversas.pop(
                                st.session_state.conversa_id, []
                            )

                        st.session_state.conversa_id = novo_nome

                st.rerun()



    # Mostrar hist√≥rico 
    st.divider()
    for msg in st.session_state.chat_history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # ‚úÖ Mostrar conte√∫do da mem√≥ria, se debug_mode estiver ativo
    if debug_mode:
        memory = st.session_state.memory
        st.divider()

        st.sidebar.subheader("üìú Mem√≥ria formatada para o modelo (roles corrigidos)")
        for i, m in enumerate(memory.chat_memory.messages):
            role = "user" if m.type == "human" else "assistant"
            st.sidebar.markdown(f"**{i+1}. {role.upper()}**")
            st.sidebar.code(m.content, language="markdown")

        with st.expander("üß† Conte√∫do atual da mem√≥ria (LangChain)"):
            if not memory.chat_memory.messages:
                st.markdown("‚ùå *A mem√≥ria est√° vazia. Nada foi guardado.*")
            else:
                st.subheader("üì® LangChain Memory")
                for i, m in enumerate(memory.chat_memory.messages):
                    st.markdown(f"**{i+1}. {m.type.upper()}**")
                    st.code(m.content, language="markdown")

                st.subheader("üß± Representa√ß√£o bruta da mem√≥ria (debug avan√ßado)")
                for i, m in enumerate(memory.chat_memory.messages):
                    st.code(repr(m), language="python")

                if any(m.content.strip() == "" for m in memory.chat_memory.messages):
                    st.warning("‚ö†Ô∏è H√° mensagens com conte√∫do vazio na mem√≥ria.")

                if st.session_state.chat_history:
                    st.subheader("üßæ Hist√≥rico Visual (chat_history)")
                    for i, m in enumerate(st.session_state.chat_history):
                        st.markdown(f"**{i+1}. {m['role'].upper()}**")
                        st.code(m["content"], language="markdown")

                # üßπ Bot√£o para limpar tudo
                if st.button("üßπ Limpar mem√≥ria da sess√£o"):
                    memory.clear()
                    st.session_state.chat_history = []
                    st.session_state.mensagens = []
                    st.success("‚úÖ Mem√≥ria e hist√≥rico visual limpos com sucesso.")
                    st.rerun()

        # üì§ Mostrar prompt enviado ao modelo
        if "mensagens" in st.session_state and st.session_state.mensagens:
            st.sidebar.subheader("üì§ Prompt enviado ao modelo (√∫ltima intera√ß√£o)")
            for i, m in enumerate(st.session_state.mensagens):
                st.sidebar.markdown(f"**{i+1}. {m['role'].upper()}**")
                st.sidebar.code(m["content"], language="markdown")
        else:
            st.sidebar.markdown("‚ö†Ô∏è *O prompt enviado ao modelo ainda n√£o foi guardado.*")



    # Limpar input se sinalizador estiver ativo
    if "reset_input" in st.session_state and st.session_state.reset_input:
        st.session_state.pergunta_input = ""
        st.session_state.reset_input = False

    # Input com submit via Enter
    st.divider()
    with st.form(key="form_pergunta"):
        user_input = st.text_input(
            "‚úèÔ∏è Faz a tua pergunta:",
            placeholder="Escreve aqui a tua quest√£o...",
            key="pergunta_input"
        )
        submitted = st.form_submit_button("Enviar")

        if submitted and user_input.strip():
            pergunta = user_input
            st.session_state.primeira_interacao = False

            st.session_state.chat_history.append({"role": "user", "content": pergunta})
            resposta = responder_pergunta(pergunta)
            st.session_state.chat_history.append({"role": "assistant", "content": resposta})

            # ‚úÖ Renomear conversa se ainda estiver com nome gen√©rico
            if st.session_state.conversa_id.startswith("conversa_"):
                novo_nome = gerar_nome_conversa_baseado_na_pergunta(pergunta)

                pasta_user = f"historico_data/user_{st.session_state.user_id}"
                caminho_antigo = os.path.join(pasta_user, f"{st.session_state.conversa_id}.json")
                caminho_novo = os.path.join(pasta_user, f"{novo_nome}.json")

                if not os.path.exists(caminho_novo):
                    os.rename(caminho_antigo, caminho_novo)

                    if "conversas" in st.session_state:
                        st.session_state.conversas[novo_nome] = st.session_state.conversas.pop(
                            st.session_state.conversa_id, []
                        )

                    st.session_state.conversa_id = novo_nome

            st.session_state.reset_input = True
            st.rerun()


        # ‚ùå VERS√ÉO ANTIGA (com erro, deixada aqui comentada como refer√™ncia)
        # if submitted and user_input.strip():
        #     st.session_state.primeira_interacao = False
        #     st.session_state.chat_history.append({"role": "user", "content": user_input})
        #     resposta = responder_pergunta(user_input)
        #     st.session_state.chat_history.append({"role": "assistant", "content": resposta})
        #     st.session_state.pergunta_input = ""  # limpa a caixa (isto d√° erro)
        #     st.rerun()




# ‚Äî‚Äî‚Äî TAB 2: Membros ‚Äî‚Äî‚Äî
with tab2:
    st.title("üë• Membros da Viagem")

    membros = [
        {"nome": "Jo√£o Gomes", "alcunha": "Ex da Ana Malta/TVDE Indiano", "ficheiro": "jota.jpeg"},
        {"nome": "Andr√© Marques", "alcunha": "IsoleOne", "ficheiro": "andre.jpeg"},
        {"nome": "Diogo Miranda", "alcunha": "Cara de Ovo", "ficheiro": "diogo.jpeg"},
        {"nome": "Rodrigo Miranda", "alcunha": "Rei", "ficheiro": "rodrigo.jpeg"},
        {"nome": "Rodrigo Pato", "alcunha": "Monsieur Ralph / Paty", "ficheiro": "pato.jpeg"},
        {"nome": "Vasco Machado", "alcunha": "Dumbo/Paty/Um dos Nha Bodi", "ficheiro": "vasco.jpeg"},
    ]

    cols = st.columns(3)


    # Mostrar os membros em linhas de 3
    for i in range(0, len(membros), 3):
        cols = st.columns(3)
        for j, membro in enumerate(membros[i:i+3]):
            with cols[j]:
                with st.container():
                    st.markdown(
                        f"""
                        <div style='
                            background-color: #2b2b2b;
                            padding: 15px;
                            border-radius: 12px;
                            text-align: center;
                            box-shadow: 0 4px 12px rgba(0,0,0,0.3);
                            margin-bottom: 20px;
                        '>
                        """,
                        unsafe_allow_html=True
                    )

                    try:
                        img = Image.open(f"imagens/{membro['ficheiro']}")
                        st.markdown(
                            f"""
                            <img src="data:image/jpeg;base64,{img_to_base64(img)}"
                                style="width: 150px; border-radius: 10px; transition: 0.3s ease;
                                        box-shadow: 0 0 5px rgba(255,255,255,0.1);"
                                onmouseover="this.style.transform='scale(1.05)'; 
                                            this.style.boxShadow='0 0 15px rgba(138,255,255,0.5)'"
                                onmouseout="this.style.transform='scale(1)'; 
                                            this.style.boxShadow='0 0 5px rgba(255,255,255,0.1)'"
                            />
                            """,
                            unsafe_allow_html=True
                        )
                    except:
                        st.error(f"‚ùå Imagem n√£o encontrada: {membro['ficheiro']}")

                    st.markdown(f"**{membro['nome']}**")
                    st.markdown(f"<span style='color: #8ef; font-style: italic;'>{membro['alcunha']}</span>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)


